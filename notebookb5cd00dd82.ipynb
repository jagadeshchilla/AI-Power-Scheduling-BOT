{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install pandas numpy faker","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport uuid\nfrom faker import Faker\nfrom datetime import datetime, timedelta\nimport json\nimport os\n\nclass SchedulingBotDatasetGenerator:\n    def __init__(self, num_records=10000):\n        self.num_records = num_records\n        self.fake = Faker()\n        self.used_ids = set()\n\n    def generate_unique_id(self):\n        \"\"\"Generate a unique ID\"\"\"\n        while True:\n            new_id = str(uuid.uuid4())\n            if new_id not in self.used_ids:\n                self.used_ids.add(new_id)\n                return new_id\n\n    def generate_conversation_dataset(self):\n        \"\"\"\n        Generate comprehensive conversation dataset\n        \"\"\"\n        conversation_data = []\n        \n        # Predefined lists for rich data generation\n        intents = [\n            'offer_availability', \n            'schedule_meeting', \n            'reschedule', \n            'cancel_meeting', \n            'request_time_slot'\n        ]\n        \n        speakers = ['Candidate', 'Recruiter', 'Bot']\n        timezones = ['UTC-5', 'UTC-8', 'UTC+1', 'UTC+5', 'UTC+8']\n        \n        # Advanced message templates\n        message_templates = self._generate_message_templates()\n        \n        for _ in range(self.num_records):\n            # Generate unique session and message IDs\n            session_id = self.generate_unique_id()\n            message_id = self.generate_unique_id()\n            \n            # Create conversation record\n            conversation_record = {\n                'Message_ID': message_id,\n                'Session_ID': session_id,\n                'Speaker': random.choice(speakers),\n                'Timestamp': self._generate_timestamp(),\n                'Raw_Text': random.choice(message_templates),\n                'Parsed_Date': self._generate_date(),\n                'Parsed_Time': self.fake.time(),\n                'Intent_Label': random.choice(intents),\n                'Entities': self._generate_entities(timezones),\n                'Context': self.fake.sentence(),\n                'Language': random.choice(['en', 'es', 'fr', 'de']),\n                'Sentiment': random.choice(['positive', 'neutral', 'negative'])\n            }\n            \n            conversation_data.append(conversation_record)\n        \n        return pd.DataFrame(conversation_data)\n\n    def generate_calendar_dataset(self):\n        \"\"\"\n        Generate comprehensive calendar dataset\n        \"\"\"\n        calendar_data = []\n        \n        # Predefined lists for rich data generation\n        meeting_types = [\n            'Technical Interview', \n            'HR Screening', \n            'Final Round', \n            'Initial Discussion',\n            'Follow-up Meeting',\n            'Project Presentation'\n        ]\n        \n        availability_statuses = [\n            'Available', 'Booked', 'Tentative', \n            'Blocked', 'Pending Confirmation'\n        ]\n        \n        locations = [\n            'Zoom', 'Google Meet', 'Microsoft Teams', \n            'In-Person', 'Hybrid', 'Phone Call'\n        ]\n        \n        for _ in range(self.num_records):\n            # Generate unique IDs\n            event_id = self.generate_unique_id()\n            recruiter_id = self.generate_unique_id()\n            candidate_id = self.generate_unique_id()\n            \n            # Generate timestamps\n            start_time = self._generate_datetime()\n            end_time = start_time + timedelta(minutes=random.randint(30, 120))\n            \n            # Create calendar record\n            calendar_record = {\n                'Event_ID': event_id,\n                'Recruiter_ID': recruiter_id,\n                'Candidate_ID': candidate_id,\n                'Date': start_time.date(),\n                'Start_Time': start_time.time(),\n                'End_Time': end_time.time(),\n                'Availability_Status': random.choice(availability_statuses),\n                'Meeting_Duration': (end_time - start_time).total_seconds() / 60,\n                'TimeZone': random.choice(['UTC-5', 'UTC-8', 'UTC+1', 'UTC+5']),\n                'Meeting_Type': random.choice(meeting_types),\n                'Invite_Status': random.choice([\n                    'Sent', 'Pending', 'Accepted', 'Declined'\n                ]),\n                'Location': random.choice(locations),\n                'Department': random.choice([\n                    'Engineering', 'Sales', 'Marketing', \n                    'Product', 'Customer Success'\n                ]),\n                'Priority': random.choice(['High', 'Medium', 'Low'])\n            }\n            \n            calendar_data.append(calendar_record)\n        \n        return pd.DataFrame(calendar_data)\n\n    def _generate_message_templates(self):\n        \"\"\"\n        Generate advanced message templates with contextual variations\n        \"\"\"\n        templates = [\n            # Availability Offers\n            \"I'm available next {day} between {start_time} and {end_time}\",\n            \"My schedule is open on {day} from {start_time} to {end_time}\",\n            \"I have free slots on {day} around {time_range}\",\n            \n            # Interview Requests\n            \"Can we schedule a {duration} interview for {day}?\",\n            \"I'm looking to book a {duration} meeting next week\",\n            \"Would you have time for a {duration} discussion?\",\n            \n            # Specific Scheduling Requests\n            \"I prefer morning/afternoon meetings on {day}\",\n            \"Are there any open slots for a {meeting_type} next {day_period}?\",\n            \"Looking for a {meeting_type} interview this week\",\n            \n            # Rescheduling and Modifications\n            \"I need to reschedule our previous meeting\",\n            \"Can we move our discussion to a different time?\",\n            \"My availability has changed. Let's find a new slot.\",\n            \n            # Time Zone and Flexibility Considerations\n            \"I'm in {timezone}. What times work best for you?\",\n            \"Can we accommodate my {timezone} schedule?\",\n        ]\n        \n        def format_template(template):\n            days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n            time_ranges = ['morning', 'afternoon', 'evening']\n            start_times = ['9:00 AM', '10:30 AM', '2:00 PM', '3:30 PM']\n            end_times = ['11:00 AM', '12:30 PM', '4:00 PM', '5:30 PM']\n            durations = ['30-minute', '45-minute', '60-minute']\n            meeting_types = ['Technical', 'HR', 'Initial Screening', 'Final Round']\n            day_periods = ['week', 'weekend']\n            timezones = ['EST', 'PST', 'CST', 'UTC']\n            \n            return template.format(\n                day=random.choice(days),\n                start_time=random.choice(start_times),\n                end_time=random.choice(end_times),\n                time_range=random.choice(time_ranges),\n                duration=random.choice(durations),\n                meeting_type=random.choice(meeting_types),\n                day_period=random.choice(day_periods),\n                timezone=random.choice(timezones)\n            )\n        \n        return [format_template(template) for template in templates]\n\n    def _generate_entities(self, timezones):\n        \"\"\"\n        Generate structured entities for NLP processing\n        \"\"\"\n        return json.dumps({\n            'Duration': random.choice(['30 min', '45 min', '60 min']),\n            'TimeZone': random.choice(timezones),\n            'Day': random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']),\n            'TimeSlot': random.choice(['Morning', 'Afternoon', 'Evening'])\n        })\n\n    def _generate_timestamp(self):\n        \"\"\"Generate a unique timestamp\"\"\"\n        return self.fake.date_time_between(start_date='-1y', end_date='+1y')\n\n    def _generate_date(self):\n        \"\"\"Generate a unique date\"\"\"\n        return self.fake.date_between(start_date='-1y', end_date='+1y')\n\n    def _generate_datetime(self):\n        \"\"\"Generate a unique datetime\"\"\"\n        return self.fake.date_time_between(start_date='-1y', end_date='+1y')\n\n    def save_datasets(self, output_dir='scheduling_bot_datasets'):\n        \"\"\"\n        Save generated datasets to CSV\n        \"\"\"\n        # Create output directory\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Generate datasets\n        conversation_df = self.generate_conversation_dataset()\n        calendar_df = self.generate_calendar_dataset()\n        \n        # Save paths\n        conversation_path = os.path.join(output_dir, 'conversation_dataset.csv')\n        calendar_path = os.path.join(output_dir, 'calendar_dataset.csv')\n        \n        # Save to CSV\n        conversation_df.to_csv(conversation_path, index=False)\n        calendar_df.to_csv(calendar_path, index=False)\n        \n        print(f\"Conversation Dataset saved to: {conversation_path}\")\n        print(f\"Calendar Dataset saved to: {calendar_path}\")\n        \n        return conversation_df, calendar_df\n\ndef validate_dataset(df, dataset_type):\n    \"\"\"\n    Validate generated dataset\n    \"\"\"\n    print(f\"\\n{dataset_type} Dataset Validation:\")\n    \n    # Check for missing values\n    print(\"Missing Values:\")\n    print(df.isnull().sum())\n    \n    # Check unique IDs\n    id_columns = {\n        'Conversation': ['Message_ID', 'Session_ID'],\n        'Calendar': ['Event_ID', 'Recruiter_ID', 'Candidate_ID']\n    }\n    \n    for col in id_columns.get(dataset_type, []):\n        unique_count = df[col].nunique()\n        total_count = len(df)\n        print(f\"{col} Unique Check: {unique_count == total_count}\")\n    \n    # Basic statistical overview\n    print(\"\\nDataset Overview:\")\n    print(df.info())\n    \n    # Distribution of categorical columns\n    categorical_columns = {\n        'Conversation': ['Speaker', 'Intent_Label', 'Language', 'Sentiment'],\n        'Calendar': ['Meeting_Type', 'Availability_Status', 'Invite_Status', 'Location']\n    }\n    \n    for col in categorical_columns.get(dataset_type, []):\n        print(f\"\\n{col} Distribution:\")\n        print(df[col].value_counts(normalize=True))\n\ndef main():\n    # Set random seed for reproducibility\n    np.random.seed(42)\n    random.seed(42)\n    \n    # Initialize dataset generator\n    generator = SchedulingBotDatasetGenerator(num_records=10000)\n    \n    # Generate and save datasets\n    conversation_df, calendar_df = generator.save_datasets()\n    \n    # Validate datasets\n    validate_dataset(conversation_df, 'Conversation')\n    validate_dataset(calendar_df, 'Calendar')\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Initial Dataset Examination and Loading\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\n\n# Load the datasets\nprint(\"Step 1: Loading Datasets\")\nconversation_df = pd.read_csv('scheduling_bot_datasets/conversation_dataset.csv')\ncalendar_df = pd.read_csv('scheduling_bot_datasets/calendar_dataset.csv')\n\n# Display basic information about the datasets\nprint(\"\\nConversation Dataset Information:\")\nprint(conversation_df.info())\n\nprint(\"\\nCalendar Dataset Information:\")\nprint(calendar_df.info())\n\n# Step 2: Data Quality Check\nprint(\"\\nStep 2: Data Quality Check\")\n\n# Check for missing values\nprint(\"\\nMissing Values in Conversation Dataset:\")\nprint(conversation_df.isnull().sum())\n\nprint(\"\\nMissing Values in Calendar Dataset:\")\nprint(calendar_df.isnull().sum())\n\n# Check for duplicate IDs\nprint(\"\\nDuplicate Message IDs in Conversation Dataset:\")\nprint(conversation_df['Message_ID'].duplicated().sum())\n\nprint(\"\\nDuplicate Event IDs in Calendar Dataset:\")\nprint(calendar_df['Event_ID'].duplicated().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conversation_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"calendar_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nimport traceback\n\n# Enhanced Logging Function\ndef log_error(e):\n    \"\"\"\n    Log errors with detailed traceback\n    \"\"\"\n    print(\"An error occurred:\")\n    print(str(e))\n    print(\"\\nDetailed Traceback:\")\n    traceback.print_exc()\n\n# Step 1: Initial Dataset Examination and Loading\ndef load_and_examine_datasets():\n    try:\n        print(\"Step 1: Loading Datasets\")\n        \n        # Load datasets with error handling\n        try:\n            conversation_df = pd.read_csv('scheduling_bot_datasets/conversation_dataset.csv')\n            calendar_df = pd.read_csv('scheduling_bot_datasets/calendar_dataset.csv')\n        except FileNotFoundError as e:\n            print(f\"Error: Dataset file not found. {e}\")\n            return None, None\n        except pd.errors.EmptyDataError:\n            print(\"Error: One or both datasets are empty.\")\n            return None, None\n        \n        # Display basic information about the datasets\n        print(\"\\nConversation Dataset Information:\")\n        print(conversation_df.info())\n\n        print(\"\\nCalendar Dataset Information:\")\n        print(calendar_df.info())\n        \n        return conversation_df, calendar_df\n    \n    except Exception as e:\n        log_error(e)\n        return None, None\n\n# Step 2: Data Quality Check\ndef perform_data_quality_check(conversation_df, calendar_df):\n    try:\n        print(\"\\nStep 2: Data Quality Check\")\n\n        # Check for missing values\n        print(\"\\nMissing Values in Conversation Dataset:\")\n        print(conversation_df.isnull().sum())\n\n        print(\"\\nMissing Values in Calendar Dataset:\")\n        print(calendar_df.isnull().sum())\n\n        # Check for duplicate IDs\n        print(\"\\nDuplicate Message IDs in Conversation Dataset:\")\n        print(conversation_df['Message_ID'].duplicated().sum())\n\n        print(\"\\nDuplicate Event IDs in Calendar Dataset:\")\n        print(calendar_df['Event_ID'].duplicated().sum())\n    \n    except Exception as e:\n        log_error(e)\n\n# Step 3: Exploratory Data Analysis\ndef perform_exploratory_data_analysis(conversation_df, calendar_df):\n    try:\n        print(\"\\nStep 3: Exploratory Data Analysis\")\n\n        # Ensure datetime conversion\n        conversation_df['Timestamp'] = pd.to_datetime(conversation_df['Timestamp'])\n        calendar_df['Date'] = pd.to_datetime(calendar_df['Date'])\n\n        # Create a multi-plot figure for comprehensive visualization\n        fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n\n        # Conversation Dataset Categorical Variables\n        conversation_df['Intent_Label'].value_counts().plot(\n            kind='bar', \n            ax=axs[0, 0], \n            title='Intent Label Distribution'\n        )\n        axs[0, 0].set_xticklabels(axs[0, 0].get_xticklabels(), rotation=45, ha='right')\n\n        conversation_df['Speaker'].value_counts().plot(\n            kind='pie', \n            ax=axs[0, 1], \n            autopct='%1.1f%%', \n            title='Speaker Distribution'\n        )\n\n        # Calendar Dataset Categorical Variables\n        calendar_df['Meeting_Type'].value_counts().plot(\n            kind='bar', \n            ax=axs[1, 0], \n            title='Meeting Type Distribution'\n        )\n        axs[1, 0].set_xticklabels(axs[1, 0].get_xticklabels(), rotation=45, ha='right')\n\n        calendar_df['Availability_Status'].value_counts().plot(\n            kind='pie', \n            ax=axs[1, 1], \n            autopct='%1.1f%%', \n            title='Availability Status Distribution'\n        )\n\n        plt.tight_layout()\n        plt.savefig('categorical_distribution.png')\n        plt.close()\n\n        # Temporal Analysis Figure\n        fig, axs = plt.subplots(2, 2, figsize=(20, 15))\n\n        # Conversations by Hour of Day\n        conversation_df['Timestamp'].dt.hour.value_counts().sort_index().plot(\n            kind='bar', \n            ax=axs[0, 0], \n            title='Conversations by Hour of Day'\n        )\n        axs[0, 0].set_xlabel('Hour')\n        axs[0, 0].set_ylabel('Number of Conversations')\n\n        # Meetings by Day of Week\n        calendar_df['Date'].dt.day_name().value_counts().plot(\n            kind='bar', \n            ax=axs[0, 1], \n            title='Meetings by Day of Week'\n        )\n        axs[0, 1].set_xticklabels(axs[0, 1].get_xticklabels(), rotation=45, ha='right')\n\n        # Text Length Analysis\n        conversation_df['Raw_Text'].str.len().plot(\n            kind='hist', \n            bins=30, \n            ax=axs[1, 0], \n            title='Text Length Distribution'\n        )\n        axs[1, 0].set_xlabel('Text Length')\n        axs[1, 0].set_ylabel('Frequency')\n\n        # Meeting Duration Distribution\n        calendar_df['Meeting_Duration'].plot(\n            kind='hist', \n            bins=30, \n            ax=axs[1, 1], \n            title='Meeting Duration Distribution'\n        )\n        axs[1, 1].set_xlabel('Meeting Duration')\n        axs[1, 1].set_ylabel('Frequency')\n\n        plt.tight_layout()\n        plt.savefig('temporal_analysis.png')\n        plt.close()\n\n    except Exception as e:\n        log_error(e)\n\n# Step 4: Data Preprocessing\ndef preprocess_data(conversation_df, calendar_df):\n    try:\n        print(\"\\nStep 4: Data Preprocessing\")\n\n        # Preprocessing Conversation Dataset\n        def preprocess_conversation_data(df):\n            # Convert Timestamp\n            df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n            \n            # Extract additional time-based features\n            df['Hour_of_Day'] = df['Timestamp'].dt.hour\n            df['Day_of_Week'] = df['Timestamp'].dt.day_name()\n            df['Month'] = df['Timestamp'].dt.month_name()\n            \n            # Text-based features\n            df['Text_Length'] = df['Raw_Text'].str.len()\n            \n            # Parse Entities with robust error handling\n            def parse_entities(entities_str):\n                try:\n                    entities = json.loads(entities_str)\n                    return pd.Series({\n                        'Duration': entities.get('Duration', ''),\n                        'TimeZone': entities.get('TimeZone', '')\n                    })\n                except (json.JSONDecodeError, TypeError):\n                    return pd.Series({'Duration': '', 'TimeZone': ''})\n            \n            entities_df = df['Entities'].apply(parse_entities)\n            df = pd.concat([df, entities_df], axis=1)\n            \n            return df\n\n        # Preprocessing Calendar Dataset\n        def preprocess_calendar_data(df):\n            # Convert Date\n            df['Date'] = pd.to_datetime(df['Date'])\n            \n            # Flexible time parsing\n            def parse_time(time_str):\n                try:\n                    # Try multiple time formats\n                    formats = ['%H:%M:%S', '%H:%M:%S.%f', '%I:%M %p', '%H:%M']\n                    for fmt in formats:\n                        try:\n                            return pd.to_datetime(time_str, format=fmt).time()\n                        except:\n                            continue\n                    return None\n                except Exception as e:\n                    print(f\"Time parsing error: {e}\")\n                    return None\n\n            # Parse Start and End Times\n            df['Start_Time'] = df['Start_Time'].apply(parse_time)\n            df['End_Time'] = df['End_Time'].apply(parse_time)\n            \n            # Calculate Meeting Duration\n            def calculate_duration(row):\n                try:\n                    start = pd.to_datetime(row['Start_Time'].strftime('%H:%M:%S'))\n                    end = pd.to_datetime(row['End_Time'].strftime('%H:%M:%S'))\n                    \n                    # Handle times crossing midnight\n                    if end < start:\n                        end += pd.Timedelta(days=1)\n                    \n                    return (end - start).total_seconds() / 60\n                except Exception as e:\n                    print(f\"Duration calculation error: {e}\")\n                    return None\n\n            df['Meeting_Duration_Minutes'] = df.apply(calculate_duration, axis=1)\n            \n            # Fill NaN durations with mean\n            df['Meeting_Duration_Minutes'].fillna(\n                df['Meeting_Duration_Minutes'].mean(), \n                inplace=True\n            )\n            \n            # Extract additional time-based features\n            df['Day_of_Week'] = df['Date'].dt.day_name()\n            df['Month'] = df['Date'].dt.month_name()\n            \n            return df\n\n        # Apply preprocessing\n        preprocessed_conversation_df = preprocess_conversation_data(conversation_df.copy())\n        preprocessed_calendar_df = preprocess_calendar_data(calendar_df.copy())\n\n        return preprocessed_conversation_df, preprocessed_calendar_df\n\n    except Exception as e:\n        log_error(e)\n        return None, None\n\n# Main Execution Function\ndef main():\n    try:\n        # Step 1: Load Datasets\n        conversation_df, calendar_df = load_and_examine_datasets()\n        \n        if conversation_df is None or calendar_df is None:\n            print(\"Failed to load datasets. Exiting.\")\n            return\n\n        # Step 2: Data Quality Check\n        perform_data_quality_check(conversation_df, calendar_df)\n\n        # Step 3: Exploratory Data Analysis\n        perform_exploratory_data_analysis(conversation_df, calendar_df)\n\n        # Step 4: Preprocess Data\n        preprocessed_conversation_df, preprocessed_calendar_df = preprocess_data(\n            conversation_df, \n            calendar_df\n        )\n\n        if preprocessed_conversation_df is None or preprocessed_calendar_df is None:\n            print(\"Data preprocessing failed. Exiting.\")\n            return\n\n        # Step 5: Encoding Categorical Variables\n        from sklearn.preprocessing import LabelEncoder\n\n        # Conversation Dataset Encoding\n        conversation_label_encoders = {}\n        conversation_categorical_cols = [\n            'Speaker', 'Intent_Label', 'Language', 'Sentiment'\n        ]\n\n        for col in conversation_categorical_cols:\n            le = LabelEncoder()\n            preprocessed_conversation_df[f'{col}_Encoded'] = le.fit_transform(\n                preprocessed_conversation_df[col]\n            )\n            conversation_label_encoders[col] = le\n\n        # Calendar Dataset Encoding\n        calendar_label_encoders = {}\n        calendar_categorical_cols = [\n            'Availability_Status', 'Meeting_Type', 'Invite_Status', \n            'Location', 'Department', 'Priority'\n        ]\n\n        for col in calendar_categorical_cols:\n            le = LabelEncoder()\n            preprocessed_calendar_df[f'{col}_Encoded'] = le.fit_transform(\n                preprocessed_calendar_df[col]\n            )\n            calendar_label_encoders[col] = le\n\n        # Step 6: Feature Selection for Machine Learning\n        conversation_ml_features = [\n            'Hour_of_Day', 'Text_Length', 'Speaker_Encoded', \n            'Language_Encoded', 'Sentiment_Encoded'\n        ]\n\n        calendar_ml_features = [\n            'Meeting_Duration_Minutes', 'Availability_Status_Encoded', \n            'Location_Encoded', 'Department_Encoded'\n        ]\n\n        # Prepare features and targets\n        X_intent = preprocessed_conversation_df[conversation_ml_features]\n        y_intent = preprocessed_conversation_df['Intent_Label_Encoded']\n\n        X_time_slot = preprocessed_calendar_df[calendar_ml_features]\n        y_time_slot = preprocessed_calendar_df['Meeting_Type_Encoded']\n\n        # Step 7: Save Preprocessed Datasets\n        preprocessed_conversation_df.to_csv(\n            'preprocessed_conversation_dataset.csv', \n            index=False\n        )\n        preprocessed_calendar_df.to_csv(\n            'preprocessed_calendar_dataset.csv', \n            index=False\n        )\n\n        # Save Label Encoders\n        import joblib\n        joblib.dump(conversation_label_encoders, 'conversation_label_encoders.pkl')\n        joblib.dump(calendar_label_encoders, 'calendar_label_encoders.pkl')\n\n        print(\"Data Preprocessing Completed Successfully!\")\n\n    except Exception as e:\n        log_error(e)\n\n# Run the main function\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport logging\nimport traceback\n\n# Sklearn Imports\nfrom sklearn.model_selection import (\n    train_test_split, \n    cross_val_score, \n    StratifiedKFold, \n    GridSearchCV\n)\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import (\n    classification_report, \n    confusion_matrix, \n    accuracy_score, \n    precision_recall_fscore_support,\n    roc_auc_score\n)\n\n# Machine Learning Algorithms\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\n# Configure Logging\nlogging.basicConfig(\n    level=logging.INFO, \n    format='%(asctime)s - %(levelname)s: %(message)s',\n    filename='ml_training.log'\n)\n\nclass SchedulingBotModelTrainer:\n    def __init__(self, conversation_df, calendar_df):\n        \"\"\"\n        Initialize the model trainer with preprocessed datasets\n        \"\"\"\n        self.conversation_df = conversation_df\n        self.calendar_df = calendar_df\n        \n        # Logging\n        self.logger = logging.getLogger(__name__)\n    \n    def prepare_ml_datasets(self):\n        \"\"\"\n        Prepare datasets for machine learning\n        \"\"\"\n        try:\n            # Intent Classification Dataset\n            intent_features = [\n                'Hour_of_Day', \n                'Text_Length', \n                'Speaker_Encoded', \n                'Language_Encoded', \n                'Sentiment_Encoded'\n            ]\n            \n            X_intent = self.conversation_df[intent_features]\n            y_intent = self.conversation_df['Intent_Label_Encoded']\n            \n            # Time Slot Prediction Dataset\n            time_slot_features = [\n                'Meeting_Duration_Minutes', \n                'Availability_Status_Encoded', \n                'Location_Encoded', \n                'Department_Encoded'\n            ]\n            \n            X_time_slot = self.calendar_df[time_slot_features]\n            y_time_slot = self.calendar_df['Meeting_Type_Encoded']\n            \n            return {\n                'intent': {\n                    'features': X_intent,\n                    'target': y_intent\n                },\n                'time_slot': {\n                    'features': X_time_slot,\n                    'target': y_time_slot\n                }\n            }\n        except Exception as e:\n            self.logger.error(f\"Error in preparing ML datasets: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def split_datasets(self, datasets):\n        \"\"\"\n        Split datasets into training and testing sets\n        \"\"\"\n        try:\n            # Intent Classification Split\n            X_intent_train, X_intent_test, y_intent_train, y_intent_test = train_test_split(\n                datasets['intent']['features'], \n                datasets['intent']['target'], \n                test_size=0.2, \n                random_state=42,\n                stratify=datasets['intent']['target']\n            )\n            \n            # Time Slot Prediction Split\n            X_time_slot_train, X_time_slot_test, y_time_slot_train, y_time_slot_test = train_test_split(\n                datasets['time_slot']['features'], \n                datasets['time_slot']['target'], \n                test_size=0.2, \n                random_state=42,\n                stratify=datasets['time_slot']['target']\n            )\n            \n            return {\n                'intent': {\n                    'X_train': X_intent_train,\n                    'X_test': X_intent_test,\n                    'y_train': y_intent_train,\n                    'y_test': y_intent_test\n                },\n                'time_slot': {\n                    'X_train': X_time_slot_train,\n                    'X_test': X_time_slot_test,\n                    'y_train': y_time_slot_train,\n                    'y_test': y_time_slot_test\n                }\n            }\n        except Exception as e:\n            self.logger.error(f\"Error in splitting datasets: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def create_model_pipeline(self, model, param_grid=None):\n        \"\"\"\n        Create a machine learning pipeline with optional hyperparameter tuning\n        \"\"\"\n        try:\n            pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                ('classifier', model)\n            ])\n            \n            # Hyperparameter tuning if param_grid is provided\n            if param_grid:\n                cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n                grid_search = GridSearchCV(\n                    pipeline, \n                    param_grid, \n                    cv=cv, \n                    scoring='accuracy', \n                    n_jobs=-1\n                )\n                return grid_search\n            \n            return pipeline\n        except Exception as e:\n            self.logger.error(f\"Error in creating model pipeline: {e}\")\n            traceback.print_exc()\n            return None\n    \n    def train_and_evaluate_models(self, split_data):\n        \"\"\"\n        Train and evaluate multiple machine learning models\n        \"\"\"\n        models = {\n            'Intent Classification': {\n                'Random Forest': (\n                    RandomForestClassifier(random_state=42),\n                    {\n                        'classifier__n_estimators': [50, 100, 200],\n                        'classifier__max_depth': [None, 10, 20]\n                    }\n                ),\n                'XGBoost': (\n                    XGBClassifier(random_state=42),\n                    {\n                        'classifier__n_estimators': [50, 100, 200],\n                        'classifier__learning_rate': [0.01, 0.1, 0.3]\n                    }\n                ),\n                'SVM': (\n                    SVC(probability=True),\n                    {\n                        'classifier__C': [0.1, 1, 10],\n                        'classifier__kernel': ['linear', 'rbf']\n                    }\n                )\n            },\n            'Time Slot Prediction': {\n                'Random Forest': (\n                    RandomForestClassifier(random_state=42),\n                    {\n                        'classifier__n_estimators': [50, 100, 200],\n                        'classifier__max_depth': [None, 10, 20]\n                    }\n                ),\n                'Gradient Boosting': (\n                    GradientBoostingClassifier(random_state=42),\n                    {\n                        'classifier__n_estimators': [50, 100, 200],\n                        'classifier__learning_rate': [0.01, 0.1, 0.3]\n                    }\n                )\n            }\n        }\n        \n        results = {}\n        \n        for task, task_models in models.items():\n            self.logger.info(f\"\\n{task} Model Evaluation:\")\n            \n            # Select appropriate split data\n            if task == 'Intent Classification':\n                X_train = split_data['intent']['X_train']\n                X_test = split_data['intent']['X_test']\n                y_train = split_data['intent']['y_train']\n                y_test = split_data['intent']['y_test']\n            else:\n                X_train = split_data['time_slot']['X_train']\n                X_test = split_data['time_slot']['X_test']\n                y_train = split_data['time_slot']['y_train']\n                y_test = split_data['time_slot']['y_test']\n            \n            task_results = {}\n            \n            for model_name, (model, param_grid) in task_models.items():\n                try:\n                    # Create pipeline with hyperparameter tuning\n                    pipeline = self.create_model_pipeline(model, param_grid)\n                    \n                    # Train model\n                    pipeline.fit(X_train, y_train)\n                    \n                    # Best parameters and model\n                    if hasattr(pipeline, 'best_params_'):\n                        self.logger.info(f\"Best Parameters for {model_name}: {pipeline.best_params_}\")\n                        best_model = pipeline.best_estimator_\n                    else:\n                        best_model = pipeline\n                    \n                    # Predictions\n                    y_pred = best_model.predict(X_test)\n                    \n                    # Detailed Evaluation\n                    accuracy = accuracy_score(y_test, y_pred)\n                    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n                    \n                    # Additional Metrics\n                    try:\n                        roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test), multi_class='ovr')\n                    except:\n                        roc_auc = None\n                    \n                    # Cross-validation\n                    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n                    \n                    # Store results\n                    task_results[model_name] = {\n                        'accuracy': accuracy,\n                        'precision': precision,\n                        'recall': recall,\n                        'f1_score': f1,\n                        'roc_auc': roc_auc,\n                        'cv_scores': cv_scores,\n                        'model': best_model\n                    }\n                    \n                    # Log results\n                    self.logger.info(f\"\\n{model_name} Results:\")\n                    self.logger.info(f\"Accuracy: {accuracy:.4f}\")\n                    self.logger.info(f\"Precision: {precision:.4f}\")\n                    self.logger.info(f\"Recall: {recall:.4f}\")\n                    self.logger.info(f\"F1 Score: {f1:.4f}\")\n                    if roc_auc:\n                        self.logger.info(f\"ROC AUC: {roc_auc:.4f}\")\n                    self.logger.info(f\"Cross-Validation Scores: {cv_scores}\")\n                \n                except Exception as e:\n                    self.logger.error(f\"Error training {model_name}: {e}\")\n                    traceback.print_exc()\n            \n            results[task] = task_results\n        \n        return results\n    \n    def visualize_model_performance(self, results):\n        \"\"\"\n        Create visualizations of model performance\n        \"\"\"\n        for task, task_results in results.items():\n            # Prepare data for visualization\n            model_names = list(task_results.keys())\n            accuracies = [result['accuracy'] for result in task_results.values()]\n            \n            # Create bar plot\n            plt.figure(figsize=(10, 6))\n            plt.bar(model_names, accuracies)\n            plt.title(f'{task} Model Performance Comparison')\n            plt.xlabel('Models')\n            plt.ylabel('Accuracy')\n            plt.xticks(rotation=45)\n            plt.tight_layout()\n            plt.savefig(f'{task.lower().replace(\" \", \"_\")}_model_performance.png')\n            plt.close()\n    \n    def save_best_models(self, results):\n        \"\"\"\n        Save the best performing models\n        \"\"\"\n        for task, task_results in results.items():\n            # Find best model based on accuracy\n            best_model_name = max(task_results, key=lambda x: task_results[x]['accuracy'])\n            best_model = task_results[best_model_name]['model']\n            \n            # Save model\n            joblib.dump(best_model, f'best_{task.lower().replace(\" \", \"_\")}_model.pkl')\n            self.logger.info(f\"Best {task} model saved.\")\n\ndef main():\n    try:\n        # Load Preprocessed Data\n        logging.info(\"Loading Preprocessed Datasets\")\n        conversation_df = pd.read_csv('/kaggle/working/preprocessed_conversation_dataset.csv')\n        calendar_df = pd.read_csv('/kaggle/working/preprocessed_calendar_dataset.csv')\n        \n        # Initialize Model Trainer\n        trainer = SchedulingBotModelTrainer(conversation_df, calendar_df)\n        \n        # Prepare Datasets\n        ml_datasets = trainer.prepare_ml_datasets()\n        \n        # Split Datasets\n        split_data = trainer.split_datasets(ml_datasets)\n        \n        # Train and Evaluate Models\n        model_results = trainer.train_and_evaluate_models(split_data)\n        \n        # Visualize Model Performance\n        trainer.visualize_model_performance(model_results)\n        \n        # Save Best Models\n        trainer.save_best_models(model_results)\n        \n        logging.info(\"Model Training and Evaluation Completed Successfully!\")\n    \n    except Exception as e:\n        logging.error(f\"Error in main execution: {e}\")\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}